[
  {
    "objectID": "labdocs/table1_gtsummary.html",
    "href": "labdocs/table1_gtsummary.html",
    "title": "Baseline Characteristics Table",
    "section": "",
    "text": "A common task for us is putting together a baseline characteristics table, or “Table 1.” This is usually painful to do by hand, particularly when you often have to do it multiple times whenever a small change is made to a cohort. The following describes one way of automating much of this, using R and the tbl_summary() function in the gtsummary package."
  },
  {
    "objectID": "labdocs/table1_gtsummary.html#packages",
    "href": "labdocs/table1_gtsummary.html#packages",
    "title": "Baseline Characteristics Table",
    "section": "Packages",
    "text": "Packages\n\n{tidyverse}\nIf you’re not already familiar with it, I think you’ll find {tidyverse} extremely helpful - it is actually a bundle of packages centered around ‘tidy’ data, which is just a fancy description for data that take the form of 3 rules:\n\nEach variable has its own column\nEach observation has its own row.\nEach value has its own cell.\n\nOutside of time-varying analyses, that’s pretty much exactly the type of analytic dataset we are creating for most of our work.\n\n\n{gtsummary}\nIn addition to tidyverse, for the baseline characteristics table, the package {gtsummary} gets us pretty close to a final output. If you want to read more, see it’s vignette at the above link. {gtsummary} is based on R Studio’s {gt} package which is quite a nice package and has some other really cool extensions (e.g., {gtExtras})\n\n\n{haven}\n{haven} is a tidyverse package that reads SAS datasets directly. Particularly helpful for us since most of our initial data wrangling has to take place in SAS. If you haven’t noticed already, base R is not particularly good at handling huge datasets (absent a lot of memory resources on a VM) because it completely stores them in memory. If you really want to try though, {arrow} may be your best bet.\n\n\n{labelled}\n{labelled} allows us to apply a label to each variable as an “attribute.” Gtsummary will use these labels in the table output instead of variable/column names.\n\n\n{flextable}\n{flextable} will be used to get the final output into a word document.\nLet’s load the packages (you’ll probably get a lot of red text warning you about ‘masked’ objects – don’t sweat them).\n\n# if you need to install, you would use:\n# install.packages(\"gtsummary\")\n# or\n# install.packages(c(\"tidyverse\", \"gtsummary\"))\n\n# load the relevant packages\nlibrary(tidyverse)\nlibrary(gtsummary)\nlibrary(haven)\nlibrary(labelled)\nlibrary(flextable)"
  },
  {
    "objectID": "labdocs/table1_gtsummary.html#load-the-data",
    "href": "labdocs/table1_gtsummary.html#load-the-data",
    "title": "Baseline Characteristics Table",
    "section": "Load the data",
    "text": "Load the data\nHere’s where you load the SAS dataset, using the haven package.\n\n\n\n\n\n\nSome very basic R coding format.\n\n\n\nAt the most basic level, R expects you to assign values to objects. The object can be thought of as a symbol (or name) allowing you to access the value inside. The value can be any number of things, including a vector of length one, a list, a data frame (a dataset that is a special type of list), a function, and other things. The basic formatting is object <- value , which tells R to assign to ‘object’ the ‘value’ using the <- assignment operator (you can also use an = for the assignment operator instead). The short-cut for the assignment operator on a Mac is Option+- (option and minus sign).\n\n\nWe are going to assign a dataset, read from haven, to the object tab1 (can be any name you want it to be). To do so, we need the read_sas() function from haven, and the thing that is returned from that function is an R-readable data frame. There’s lots more options with this function than I use below, but the basics usually work well.\n\n\n\n\n\n\nTip\n\n\n\nNote that R has case-sensitive variable/column names. Make your life a lot easier and rename variables to lowercase with the rename_with() function and tolower option, i.e., rename_with(tolower) (see below)\n\n\n\n# note that the filepath needs forward slashes, or two backwards slashes,\n# e.g., \"C:/Users/.../...sas7bdat\" or \"C:\\\\Users\\\\...\\\\...sas7bdat\"\n#\n# tab1 <- read_sas(\"E:/.../.../...sas7bdat\") %>% \n#   rename_with(tolower)"
  },
  {
    "objectID": "labdocs/table1_gtsummary.html#data-wrangling-for-table1",
    "href": "labdocs/table1_gtsummary.html#data-wrangling-for-table1",
    "title": "Baseline Characteristics Table",
    "section": "Data wrangling for table1",
    "text": "Data wrangling for table1\nI already have my data loaded in an object called aim1cohort, so I’m just going to assign it the new name tab1, but you would not need to run this if you run above to load the data directly from a SAS dataset.\n\n\nCode\nload(file = \"/Users/stevensmith/Dropbox (UFL)/R Projects/K01-Initial_Antihtn_Prescribing/data/aim1cohort.rda\")\n\ntab1 <- aim1cohort \n\n\n\n\n\n\n\n\nA note on ’pipe’s\n\n\n\nOne thing that’s worth reading up on the %>% “pipe”. See this here or here. It’s a tidyverse thing originally coming from the magrittr package. Because it’s quite popular, Base R has now incorporated its own pipe now which does almost exactly the same thing, but looks like this: |>. You can use ctrl+shift+M as a short-cut, and in your R-studio preferences, you can tell R-Studio whether to use the native pipe |> or magrittr’s %>%. Basically, it’s a way to pipe an object forward from one function to the next, as opposed to having to nest a bunch of functions within one another.\n\n\nOk, here’s the data wrangling code:\n\n# generally bad practice unless you know exactly what you're doing - you don't want to write\n# over your objects, which is what I'm doing here:  writing over tab1 with a modified tab1\ntab1 <- tab1 %>%\n  \n  # select only variables needed; ends_with() function is a nifty short cut to \n  # grab all variables whose name ends with that string\n  select(c(\"patid\",\"source\",\"age\",\"age_cat\", \"hispanic\",\"sex\",\"race\",\"index_year\", \n           ends_with(\"indicator\"), \"combined_score_num\")) %>%\n  \n  # mutate() is for creating new variables. Some of this should look pretty \n  # similar to what you're used to in SAS. \n  # c() function just combines multiple things and works similarly here to a\n  # SAS parenthetical list, i.e., race in (\"No Information\", ...)\n  mutate(race = if_else(race %in% c(\"No Information\",\"Refuse to Answer\",\"Unknown\"), \"Unknown\", race),\n         sex = if_else(sex == \"Unknow\", \"Unknown\", sex),\n         hispanic = if_else(hispanic %in% c(\"Other\",\"Refuse to Answer\", \"No Information\", \"Unknown\"), \"Unknown\", hispanic),\n         \n         # The vector type is important for what is passed on to gtsummary, because\n         # the gtsummary packages does differet things with different vector types.\n         # for example, a numeric vector gets characterized by measures of central\n         # tendency, i.e., median (IQR), or mean ± SD. A factor will get processed\n         # as a categorical variable. BUT, if you want {gtsummary} to not bother with\n         # giving you the N (%) of people without a comorbidity (e.g., CKDindicator = 0)\n         # then you want to set those as INTEGERS. \n         # Below, we set demographics as factors, because we want all levels output. \n         # But, we set the comorbidities as integers, because we only want to know the \n         # n (%) of those who have indicator=1. \n         #\n         # The following tidyverse/dplyr syntax accomplishes this quickly, by \n         # gathering all the variables we want as factors, and all the variables\n         # we want as integers, and applying the relevant function in purrr style \n         # ~lambda:\n         across(c(hispanic, sex, index_year), ~factor(.)),\n         across(ends_with(\"indicator\"), ~as.integer(.))\n  ) %>%\n  # arrange() sorts (here, by patid).\n  arrange(patid) %>%\n  # distinct() picks out distinct values, here of patid.\n  distinct(patid, .keep_all = TRUE)\n\n\n# here we go in and work on specific columns of `tab1` dataset and \n# order the levels (values) of that column in the way we want it presented in\n# the output table, using the factor() function. Basically, we're just taking \n# the column as is, and replacing it with the same data, but telling R what is \n# should be the intrinsic order of these values when R outputs anything with it. \n# (Not actually changing given values for a given observation)\ntab1$race <- factor(tab1$race, levels = c(\"American Indian or Alaska Native\",\n                                          \"Asian\",\n                                          \"Black or African American\",\n                                          \"Native Hawaiian or Other Pacific\",\n                                          \"White\",\n                                          \"Multiple Race\",\n                                          \"Other\",\n                                          \"Unknown\"))\ntab1$age_cat <- factor(tab1$age_cat, levels = c(\"<45 y\", \"45-64 y\", \">65 y\"))\ntab1$hispanic <- factor(tab1$hispanic, levels = c(\"Hispanic\", \"Not Hispanic\", \"Unknown\"))\n\n# note that above I'm using base R coding, not tidyverse syntax. \n# I could have accomplished the above with tidyverse (dplyr) syntax also:\ntab1 <- tab1 %>% \n  mutate(race = factor(race, levels = c(\"American Indian or Alaska Native\",\n                                        \"Asian\",\n                                        \"Black or African American\",\n                                        \"Native Hawaiian or Other Pacific\",\n                                        \"White\",\n                                        \"Multiple Race\",\n                                        \"Other\",\n                                        \"Unknown\")),\n         age_cat = factor(age_cat, levels = c(\"<45 y\", \"45-64 y\", \">65 y\")),\n         hispanic = factor(hispanic, levels = c(\"Hispanic\", \"Not Hispanic\", \"Unknown\")),\n         source = factor(source, levels = c(\"FLM\", \"MED\"), labels = c(\"Medicaid\", \"Medicare\")))"
  },
  {
    "objectID": "labdocs/table1_gtsummary.html#labels",
    "href": "labdocs/table1_gtsummary.html#labels",
    "title": "Baseline Characteristics Table",
    "section": "Labels",
    "text": "Labels\nHere we can apply label and unit attributes to each column. Labels will be printed (in the output table) as specified here. It would be nice if we could also create units (e.g., years for age, or mm Hg for BP), but {gtsummary} doesn’t appear to support that yet, so the work-around is to just add units to the label.\n\n# Labels\nvar_label(tab1) <- list(age = \"Age, years\",    # units added\n                        age_cat = \"Age Category\",\n                        sex = \"Sex\",\n                        race = \"Race\",\n                        hispanic = \"Ethnicity\",\n                        smokingindicator = \"Current Smoker\",\n                        diabetesindicator = \"Diabetes\",\n                        ckdindicator = \"Chronic kidney disease\",\n                        esrdindicator = \"End-stage renal disease\",\n                        hfejindicator = \"Heart failure w/ reduced EF\",\n                        chdindicator = \"Coronary heart disease\",\n                        pcrindicator = \"Prior coronary revascularization\",\n                        strokeindicator = \"Prior stroke or TIA\",\n                        padindicator = \"Peripheral arterial disease\",\n                        ascvdindicator = \"History of clinical ASCVD\",\n                        afindicator = \"Atrial fibrillation\",\n                        copdindicator = \"Chronic obstructive pulmonary disease\",\n                        asthmaindicator = \"Asthma\",\n                        depressionindicator = \"Depression\",\n                        combined_score_num = \"Combined Comorbidity Score\",\n                        statinindicator = \"Statin\",\n                        aspirinindicator = \"Aspirin\",\n                        index_year = \"Index Year\")"
  },
  {
    "objectID": "labdocs/table1_gtsummary.html#outputting-the-table",
    "href": "labdocs/table1_gtsummary.html#outputting-the-table",
    "title": "Baseline Characteristics Table",
    "section": "Outputting the table",
    "text": "Outputting the table\nHere we tell {gtsummary} to create the table, and we apply a few customizations to the table. I’ve opted here for columns stratified by source, as well as an overall column. You could also add a p-value column (with a piped add_p()) as well as other columns. See the {gtsummary} vignette at the link above for potential options.\n\ntab1 |> \n  \n  # restrict the table that is passed to the summary to only the variables needed \n  # else, tbl_summary() will create statistics for everything in the dataset. \n  select(age, age_cat, sex, race, hispanic, smokingindicator, diabetesindicator, \n         ckdindicator, esrdindicator, hfejindicator, chdindicator, pcrindicator,\n         strokeindicator, padindicator, ascvdindicator, afindicator, copdindicator,\n         asthmaindicator, depressionindicator, combined_score_num, statinindicator,\n         aspirinindicator, index_year, source) |> \n  \n  # call the tbl_summary() function with a few customized options\n  tbl_summary(\n    \n    # stratify the table by values of 'source'\n    by = source,\n    \n    # we don't really need this, b/c we already dealt with missing values above by\n    # categorizing them as \"Unknown\", but we could label them here however we wanted,\n    # e.g., \"(Missing)\".\n    missing_text = \"(Missing)\",\n    \n    # default is to provide median (IQR), but lets say we wanted mean ± standard dev.\n    statistic = list(all_continuous() ~ \"{mean} ± {sd}\")\n    ) |> \n  \n  # add a column for the entire cohort grouped together\n  add_overall() \n\n\n\n\n\n  \n  \n    \n      Characteristic\n      Overall, N = 143,0541\n      Medicaid, N = 71,7741\n      Medicare, N = 71,2801\n    \n  \n  \n    Age, years\n59 ± 22\n47 ± 14\n72 ± 20\n    Age Category\n\n\n\n        <45 y\n38,213 (27%)\n33,176 (46%)\n5,037 (7.1%)\n        45-64 y\n51,007 (36%)\n34,274 (48%)\n16,733 (23%)\n        >65 y\n53,834 (38%)\n4,324 (6.0%)\n49,510 (69%)\n    Sex\n\n\n\n        Female\n81,555 (57%)\n43,011 (60%)\n38,544 (54%)\n        Male\n61,493 (43%)\n28,760 (40%)\n32,733 (46%)\n        Unknown\n6 (<0.1%)\n3 (<0.1%)\n3 (<0.1%)\n    Race\n\n\n\n        American Indian or Alaska Native\n334 (0.2%)\n165 (0.2%)\n169 (0.2%)\n        Asian\n1,447 (1.0%)\n659 (0.9%)\n788 (1.1%)\n        Black or African American\n33,814 (24%)\n22,041 (31%)\n11,773 (17%)\n        Native Hawaiian or Other Pacific\n33 (<0.1%)\n11 (<0.1%)\n22 (<0.1%)\n        White\n70,156 (49%)\n25,304 (35%)\n44,852 (63%)\n        Multiple Race\n503 (0.4%)\n167 (0.2%)\n336 (0.5%)\n        Other\n20,555 (14%)\n11,610 (16%)\n8,945 (13%)\n        Unknown\n16,212 (11%)\n11,817 (16%)\n4,395 (6.2%)\n    Ethnicity\n\n\n\n        Hispanic\n22,680 (16%)\n13,016 (18%)\n9,664 (14%)\n        Not Hispanic\n99,483 (70%)\n44,696 (62%)\n54,787 (77%)\n        Unknown\n20,891 (15%)\n14,062 (20%)\n6,829 (9.6%)\n    Current Smoker\n32,080 (22%)\n17,035 (24%)\n15,045 (21%)\n    Diabetes\n29,633 (21%)\n13,802 (19%)\n15,831 (22%)\n    Chronic kidney disease\n17,626 (12%)\n5,111 (7.1%)\n12,515 (18%)\n    End-stage renal disease\n1,072 (0.7%)\n233 (0.3%)\n839 (1.2%)\n    Heart failure w/ reduced EF\n2,858 (2.0%)\n1,264 (1.8%)\n1,594 (2.2%)\n    Coronary heart disease\n7,940 (5.6%)\n2,742 (3.8%)\n5,198 (7.3%)\n    Prior coronary revascularization\n728 (0.5%)\n142 (0.2%)\n586 (0.8%)\n    Prior stroke or TIA\n2,292 (1.6%)\n309 (0.4%)\n1,983 (2.8%)\n    Peripheral arterial disease\n9,388 (6.6%)\n1,483 (2.1%)\n7,905 (11%)\n    History of clinical ASCVD\n17,416 (12%)\n4,229 (5.9%)\n13,187 (19%)\n    Atrial fibrillation\n10,024 (7.0%)\n1,375 (1.9%)\n8,649 (12%)\n    Chronic obstructive pulmonary disease\n7,769 (5.4%)\n4,588 (6.4%)\n3,181 (4.5%)\n    Asthma\n6,066 (4.2%)\n4,855 (6.8%)\n1,211 (1.7%)\n    Depression\n25,690 (18%)\n10,670 (15%)\n15,020 (21%)\n    Combined Comorbidity Score\n2.5 ± 3.5\n1.6 ± 2.8\n3.5 ± 3.9\n    Statin\n31,421 (22%)\n13,201 (18%)\n18,220 (26%)\n    Aspirin\n11,663 (8.2%)\n6,115 (8.5%)\n5,548 (7.8%)\n    Index Year\n\n\n\n        2012\n32 (<0.1%)\n1 (<0.1%)\n31 (<0.1%)\n        2013\n17,728 (12%)\n485 (0.7%)\n17,243 (24%)\n        2014\n31,711 (22%)\n11,457 (16%)\n20,254 (28%)\n        2015\n27,205 (19%)\n11,039 (15%)\n16,166 (23%)\n        2016\n19,283 (13%)\n10,664 (15%)\n8,619 (12%)\n        2017\n18,661 (13%)\n9,694 (14%)\n8,967 (13%)\n        2018\n8,963 (6.3%)\n8,963 (12%)\n0 (0%)\n        2019\n8,164 (5.7%)\n8,164 (11%)\n0 (0%)\n        2020\n7,520 (5.3%)\n7,520 (10%)\n0 (0%)\n        2021\n3,787 (2.6%)\n3,787 (5.3%)\n0 (0%)\n  \n  \n  \n    \n      1 Mean ± SD; n (%)\n    \n  \n\n\n\n\nThat gives us the table - and if we wanted to save to a Word document, it’s just a couple extra lines of code (everything below is exactly the same as above, except for the pipe on the third to last line of code, and the last two lines of code):\n\ntab1 |> \n  select(age, age_cat, sex, race, hispanic, smokingindicator, diabetesindicator, \n         ckdindicator, esrdindicator, hfejindicator, chdindicator, pcrindicator,\n         strokeindicator, padindicator, ascvdindicator, afindicator, copdindicator,\n         asthmaindicator, depressionindicator, combined_score_num, statinindicator,\n         aspirinindicator, index_year, source) |> \n  tbl_summary(\n    by = source,\n    missing_text = \"(Missing)\",\n    statistic = list(all_continuous() ~ \"{mean} ± {sd}\")\n    ) |> \n  add_overall() |> \n  as_flex_table() |> \n  save_as_docx(path=\"./output/table1_output_alt.docx\")\n\nVoila! This output is pretty darn close to final output! Just needs a bit of freshening up, e.g., with bolding of column headers."
  },
  {
    "objectID": "labdocs/cumulative_incidence.html",
    "href": "labdocs/cumulative_incidence.html",
    "title": "Time-to-follow-up Visualizations",
    "section": "",
    "text": "This is an example of creating cumulative incidence curves for a time-to-event outcome. Here, we’re using our Personal Hypertension Care project data from the OneFlorida research consortium.\nWe’re using three packages: tidyverse, survival, and survminer."
  },
  {
    "objectID": "labdocs/cumulative_incidence.html#data-prep",
    "href": "labdocs/cumulative_incidence.html#data-prep",
    "title": "Time-to-follow-up Visualizations",
    "section": "Data Prep",
    "text": "Data Prep\n\n# packages\nlibrary(tidyverse)\nlibrary(survival)\nlibrary(survminer)\n\n# data + a bit of cleaning/wrangling\nfu <- read_csv(\"/Users/stevensmith/Dropbox (UFL)/Manuscripts/K01/Early Patterns of Care/data/tfu_visualization.csv\",\n               show_col_types = FALSE) |> \n  rename_with(tolower) #|> \n#  replace_na(list(time_to_actual_sfu = 0))\n\n# recalculate time variable for patients w/o a second follow-up\n# fu <- fu |> \n#   mutate(t2fu = case_when(time_to_actual_ffu == 1 & \n#                             time_to_actual_sfu == 0 ~ 365 - t1fu, \n#                           TRUE ~ t2fu))\n\n# create a subset of patients with a first follow-up (for time-to-second\n# follow-up visit plot)\nfu2 <- fu |> filter(time_to_actual_ffu == 1)\n\n# create survival objects\nfut1 <- survfit(Surv(t1fu, time_to_actual_ffu) ~ source, data = fu)\nfut2 <- survfit(Surv(t2fu, time_to_actual_sfu) ~ source, data = fu2)\n\n# function for readibility\ncomma <- function(x) format(x, digits = 2, big.mark = \",\")"
  },
  {
    "objectID": "labdocs/cumulative_incidence.html#data",
    "href": "labdocs/cumulative_incidence.html#data",
    "title": "Time-to-follow-up Visualizations",
    "section": "Data",
    "text": "Data\nWe have 64,586 Medicaid patients, and 52,712 Medicare patients. We intend to compare cumulative incidence of time-to-first follow-up (first hypertension-related visit) after starting hypertension therapy. Time 0 is date that they started antihypertensive treatment."
  },
  {
    "objectID": "labdocs/cumulative_incidence.html#plotting-time-to-first-follow-up",
    "href": "labdocs/cumulative_incidence.html#plotting-time-to-first-follow-up",
    "title": "Time-to-follow-up Visualizations",
    "section": "Plotting time-to-first follow-up",
    "text": "Plotting time-to-first follow-up\nFirst, we create a cumulative incidence curve for time-to-first follow-up. The y-axis represents the cumulative proportion that had a first follow-up visit over time (x-axis). We’re going to include Medicaid and Medicare on the same plot, as well as a formal comparison (w/ p-value), but if we don’t want to directly compare the two, we could remove the p-value, or even split these up onto 2 figures. We’re using the ggsurvplot() function from the survminer package.\n\nggsurvplot(fut1, \n           data = fu, \n           fun = \"event\", \n           censor = FALSE,\n           pval = TRUE,    # set to FALSE if don't want p-value\n           pval.coord = c(270, 0.05),  # can delete this line if don't want p-value\n           risk.table = TRUE, \n           legend.title = \"Popn\", \n           legend.labs = c(\"Medicaid\", \"Medicare\"),\n           palette = c(\"#A7473A\", \"#4B5F6C\"), \n           break.time.by = 90, \n           xlab = \"Days from Initial Visit\", \n           ylab = \"Cumulative Incidence of Follow-up\",\n           axes.offset = TRUE,\n           ylim = c(0, 1),\n           tables.theme = theme_void(),\n           tables.height = 0.15,\n           tables.col = \"strata\",\n           surv.scale = \"percent\")"
  },
  {
    "objectID": "labdocs/cumulative_incidence.html#time-to-second-follow-up",
    "href": "labdocs/cumulative_incidence.html#time-to-second-follow-up",
    "title": "Time-to-follow-up Visualizations",
    "section": "Time-to-second follow-up",
    "text": "Time-to-second follow-up\nSame thing here, but now we are restricting this plot to the 28,304 Medicaid patients, and 14,608 Medicare patients that had a first-follow-up visit. And, time 0 on x-axis represents the time of their first follow-up visit, with the outcome being second follow-up visit.\n\nggsurvplot(fut2, \n           data = fu2, \n           fun = \"event\", \n           censor = FALSE,\n           pval = TRUE, \n           pval.coord = c(30, 0.8),\n           risk.table = TRUE, \n           legend.title = \"Popn\", \n           legend.labs = c(\"Medicaid\", \"Medicare\"),\n           palette = c(\"#A7473A\", \"#4B5F6C\"), \n           break.time.by = 90, \n           xlab = \"Days from First Follow-up\", \n           ylab = \"Cumulative Incidence of Second Follow-up\",\n           axes.offset = TRUE,\n           ylim = c(0, 1),\n           tables.theme = theme_void(),\n           tables.height = 0.15,\n           tables.col = \"strata\",\n           surv.scale = \"percent\")\n\n\n\n\n\n\n\n\nThese curves look surprisingly similar! Particularly since there was a fairly stark difference in time-to-first follow-up. What to make of this?"
  },
  {
    "objectID": "labdocs/writing.html",
    "href": "labdocs/writing.html",
    "title": "Manuscript Writing",
    "section": "",
    "text": "Whether writing for medical journals, patient notes, or any other professional reason, you should strive to effectively and efficiently communicate your ideas to the reader. Developing good writing skills requires effort, patience, and practice, but can dramatically improve the amount of information that a reader takes away. Moreover, in the professional realm, inefficient communication skills can lead others to infer that you are less competent as a professional.\nConsider the following examples:\nWhy were they following the patients around?\nWhat were they at the end of the study?\nAnd, these quotes, verbatim from real hospital charts:\nAlthough you can probably guess what most of these examples were trying to communicate, they fall short of effectively conveying the intended message. Hopefully, the following information is useful in developing your scientific manuscripts, and I encourage you to continue practicing these skills throughout your professional career."
  },
  {
    "objectID": "labdocs/writing.html#active-vs.-passive-construction",
    "href": "labdocs/writing.html#active-vs.-passive-construction",
    "title": "Manuscript Writing",
    "section": "Active vs. Passive Construction",
    "text": "Active vs. Passive Construction\nReaders rely on deeply-embedded syntactic preferences. For the English language, the conventional syntax that readers expect is a sentence following a subject-verb-object order. Consequently, atypical syntax will slow a reader down, increase the amount of deciphering the brain must perform, and reduce the amount of information retained. However, even with conventional subject-verb-object order, a long and complex sentence can slow down reading and increase brain activity.\nActive construction is easier to read than passive construction. With active construction, the main actor of the sentence is the grammatical subject, followed by an action verb, then the result or object.\n\n\n\n\n\n\nPassive Construction\n\n\n\n“A decision was made by the committee to proceed.”\n\nLonger\nMore indirect\nLess clear\nLess memorable\n\nMore difficult for readers to process\nConfounds the readers expectation about sentence structure\nInverts the chronological order\n\n\n\n\n\n\n\n\n\nActive Construction\n\n\n\n“The committee made a decision to proceed.”\n\nShorter\nMore efficient\nMore direct\nMore memorable\nEasier for readers to process\nReplicates the chronological order of events\nConforms to reader expectations about sentence structure\n\n\n\nThe quick litmus test for determining if your sentence is active vs. passive is to ask yourself “Who’s doing the  here?”\n\n\n\n\n\n\nExample\n\n\n\n\n\n“Additional funding can be identified through reductions in other government spending, increased Medicare taxes and in continuous improvement of care delivery processes.”\nAsk yourself: Who’s doing the identifying here? The answer is you! The sentence is essentially saying “You can identify additional funding through ….” However, the grammatical subject of the sentence is actually “Additional funding.” So, if the answer to your litmus test question isn’t the grammatical subject of the sentence, then the sentence is passive.\nNow, consider the sentence rewritten in this way: “Congress could free up additional funding by reducing other government spending, increasing Medicare taxes and promoting continuous improvement of care delivery processes.” Who is doing the freeing up of additional funds here? Congress is, and congress is the grammatical subject; thus, this sentence is constructed actively.\n\n\n\nBe careful not to assume that a passive verb will necessarily make the sentence passive. Passive construction and passive verbs aren’t the same thing. You can have active construction with a passive verb, but you cannot have passive construction with an active verb.\nFor example:\n\n“Researchers were uncertain about the significance of these unanticipated results.”\n\nPassive verb, active sentence. (Who was uncertain?)\n\n“In the studies used, some variability was discovered in the choice and definition of fatal outcomes.”\n\nPassive verb, passive sentence. (Who is doing the discovering?)\nPassive verbs are OK, but you should make your verbs portray action whenever possible. When you use active verbs, you can avoid adding in unnecessary adverbs and prepositions, which should improve the efficiency of your writing.\n\n\n\n\n\n\nPassive verbs\n\n\n\nSome commonly used passive verbs include: is, was, were, have, has, has been, and appeared"
  },
  {
    "objectID": "labdocs/writing.html#avoid-expletives",
    "href": "labdocs/writing.html#avoid-expletives",
    "title": "Manuscript Writing",
    "section": "Avoid Expletives",
    "text": "Avoid Expletives\nExpletives (“there is/there are”) invert the order of the sentence (S-V-O) that the reader is expecting. Moreover, expletives place a meaningless adverb where readers expect the subject.\nFor example,\n\n“There was some variability among studies in their choice and definition of fatal outcomes.”\n\nWhat is the grammatical subject of the sentence? Where does it fit in the sentence with regard to order? The sentence could be rewritten as “In the studies used, some variability existed in the choice and definition of fatal outcomes.” Although this version adds one word to the length, it uses the conventional S-V-O syntax, making it easier to comprehend on the first read-through."
  },
  {
    "objectID": "labdocs/writing.html#avoid-nominalizations",
    "href": "labdocs/writing.html#avoid-nominalizations",
    "title": "Manuscript Writing",
    "section": "Avoid Nominalizations",
    "text": "Avoid Nominalizations\nNominalizations are verbs turned into nouns. They often end in suffixes like –iate or –tion or they may begin with prefixes like pre-, de-, un-, in-, or re-. Nominalizations tend to be abstract and forgettable. Moreover, they rob your verb of action and lend themselves to passive sentences.\n\n\n\n\n\n\nExamples of Nominalizations\n\n\n\n\nTransformation = transform\nIdentification = identify\nUtilization = use (this one is used all the time in our world!)\nTermination = end or fire\n\n\n\nConsider the following example:\n\n“Consolidation of the data was arranged by a coordinating center”\n\nversus:\n\n“A coordinating center consolidated the data.”\n\nCan you spot the nominalization? Which sentence reads better?"
  },
  {
    "objectID": "labdocs/writing.html#avoid-abstractions",
    "href": "labdocs/writing.html#avoid-abstractions",
    "title": "Manuscript Writing",
    "section": "Avoid Abstractions",
    "text": "Avoid Abstractions\nAlways try to use actors or tangible objects as your grammatical subjects rather than abstractions. Actors make sentences more concrete and easier to remember. Furthermore, actors lend themselves to active sentences and verbs.\nConsider the following example:\n\n“Completion of this process by the nurse will allow him to have greater success and stability in the new role.”\n\nWhat is the grammatical subject of this sentence? Consider this re-write: “Once the nurse completes this process, he’ll be more successful and more stable in his new role.” Which sentence is easier to read? Also, notice the length of each sentence. Abstractions make sentences longer.\nCompare the following sentences:\n\n“The short longevity of most common restorations and especially those of resin-based composite materials calls for detailed studies of factors which may lead to an extension of the lifetime of restorations.”\n\n\n“Resin-based composite materials have relatively short longevity.”\n\n\n\n\n\n\n\nExamples of Actors\n\n\n\n\nWe/I/they/he\nThe partners\nThe recent study\nThe state of Florida\nThe business plan"
  },
  {
    "objectID": "labdocs/writing.html#avoid-isolated-pronouns",
    "href": "labdocs/writing.html#avoid-isolated-pronouns",
    "title": "Manuscript Writing",
    "section": "Avoid Isolated Pronouns",
    "text": "Avoid Isolated Pronouns\nChoose strong subjects by avoiding isolated pronouns like this, that, these, those, or it as subjects of your sentences. These pronouns introduce ambiguity to your sentence and will force your reader to backtrack in a sentence to verify their meaning.\nFor example:\n\n“This resulted in the entire department performing better than quarterly earnings forecasts.”\n\nWhat resulted in the department performing better? We don’t know without going back to the sentence before. An easy bandaid fix is simply specifying the subject: “This strategy resulted in ….”\n\n“Despite this connection, psychosocial stressors remain under-recognized as a risk factor for heart disease compared to more traditional factors such as diabetes, hyperlipidemia, and tobacco use. Previously, this may have been due to a poor understanding of the pathophysiologic mechanisms behind this mind-heart interaction.”\n\nCan you spot the isolated pro-noun? What is the isolated pronoun referring to?"
  },
  {
    "objectID": "labdocs/writing.html#place-subjects-close-to-the-beginning-of-a-sentence",
    "href": "labdocs/writing.html#place-subjects-close-to-the-beginning-of-a-sentence",
    "title": "Manuscript Writing",
    "section": "Place Subjects Close to the Beginning of a Sentence",
    "text": "Place Subjects Close to the Beginning of a Sentence\nReaders will unconsciously search for the subject of the sentence, then for the verb, before they are able to absorb other content. Consequently, readers will perceive your sentences as difficult to read and recall when you place too much information at the beginning of your sentence or between the subject and verb.\nConsider the following sentences. Which sentence is easier to follow?\n\n“A protocol specifying the baseline, follow-up, and outcome data to be collected, and the methods of analysis, was developed and given written approval by all study groups by August, 2005.”\n\n\n“By August 2005, all study groups had created and approved a protocol, specifying the baseline, follow-up, and outcome da-ta to be collected, as well as methods of analysis.”"
  },
  {
    "objectID": "labdocs/writing.html#stress-position",
    "href": "labdocs/writing.html#stress-position",
    "title": "Manuscript Writing",
    "section": "Stress Position",
    "text": "Stress Position\nThe stress position of any sentence is the end of the sentence. This concept is true for both written and spoken language. When we speak, as we near the end of a sentence, our voices ordinarily rise in pitch on the last few words to stress the ending more strongly than the beginning or middle. As a result, items named in the ends of sentences, paragraphs, and documents receive more stress than items at the beginnings and middles. Readers find the greatest impact in the information at the end and will subsequently retain the information the longest.\nThe order of impact is as follows: End > Beginning > Middle.\nInterestingly, good writers will put less than stellar information or results (or limitations) in the beginning or middle of a sentence or paragraph in order to minimize the impact of such information.\nTo maximize the impact of the stress position(s), you should avoid sentences that are too lengthy. Long sentences:\n\nProvide few opportunities for emphasis to spotlight important items for readers to remember/focus on\nCan cause readers to lose track of the main actor/action in the sentence and lose track of relationships between items, phrases, or clauses\nCan deprive writers of opportunities for introducing readers to new information via carefully designed sequences that place new information in the stress positions of sentences, followed by sentences that use as their subjects/topics the contents of the previous sentences’ stress\n\nFor example:\n\n“The aims of this study are therefore three fold. First, we will quantify the effect of multi-drug use on the functional status of the frail elderly; second, we will determine the relationship between multi-drug use and health of the frail elderly; and third, we will define the impact of multi-drug use on the QOL of the frail elderly.”\n\nCompare the above with this group of sentences:\n\n“Therefore, this study has three aims. First, we will quantify the effect of multi-drug use on the functional status of the frail elderly. Second, we will determine the relationship between multi-drug use and health of the frail elderly. And, finally, we will define the impact of multi-drug use on the QOL of the frail elderly.”"
  },
  {
    "objectID": "labdocs/writing.html#sequencing",
    "href": "labdocs/writing.html#sequencing",
    "title": "Manuscript Writing",
    "section": "Sequencing",
    "text": "Sequencing\nReaders will absorb easily even the most complex and unfamiliar information if you sequence your sentences carefully. You should always place already-introduced information at the outset of your sentences. This can be done in two ways:\n\nMake the grammatical subject in the sentence refer to item(s) in the stress position of the previous sentence. This word need not necessarily match exactly a word in the previous sentences’ stress.\nFor example:\n\n“Despite the availability of effective pharmacological and psychotherapeutic treatments, a recent community survey found that only 16% of those with panic disorder were receiving appropriate medical management . In part, this problem is a natural consequence of the phenomenology of panic disorder and panic attacks. Panic attacks are experienced as the sudden and unexplained feeling of terror that is accompanied by severe and frightening physical symptoms such as chest pain, difficulty breathing, and heart palpitations.”\n\nVersus:\n\n“Despite the availability of effective pharmacological and psychotherapeutic treatments, a recent community survey found that only 16% of those with panic disorder were receiving appropriate medical management. In part, this problem is a natural consequence of the phenomenology of panic disorder and panic attacks. Panic attacks are experienced as the sudden and unexplained feeling of terror that is accompanied by severe and frightening physical symptoms such as chest pain, difficulty breathing, and heart palpitations.”\n\nNotice the bolded sections. The second sentence refers back to the problem highlighted in the stress position of the first sentence. In this case, the subject of sentence #2 does not repeat verbatim the stress of sentence #1. In contrast, the third sentence uses sentence #2’s stress position as its grammatical subject.\nMake the grammatical subject refer back to the subject of the preceding sentence.\nFor example:\n\n“Although most of these screening tools have demonstrated good overall psychometric properties, they are quite variable in their ability to accurately detect panic disorder. For example, PRIME-MD, one of the most widely used psy-chiatric screening instruments, has been reported to identify 83% of general medical patients with”any” psychiatric disorder, but only 57% of those with panic disorder. Only a few screening tools have been designed specifically to detect panic.”\n\nNotice the continuity between the sentences: each sentence’s grammatical subject is closely related (or verbatim) to the previous sentence."
  },
  {
    "objectID": "labdocs/writing.html#use-transitions-liberally",
    "href": "labdocs/writing.html#use-transitions-liberally",
    "title": "Manuscript Writing",
    "section": "Use Transitions Liberally",
    "text": "Use Transitions Liberally\nTransitions allow you to position the information your readers are about to encounter relative to what they have already encountered. Transitions should be used frequently (i.e. every 2-3 sentences if appropriate).\n\n\nTable 1: Transition Words & Phrases\n\n\n\n\n(a) Single-Word Transitions\n\n\nAlso\nNevertheless\nConsequently\n\n\nHowever\nBecause\nSubsequently\n\n\nAlthough\nUnfortunately\nTherefore/Thus\n\n\nConversely\nFirst/Second/Third/…\nFinally\n\n\n\n\n\n\n(b) Introductory Phrase/Clause Transitions\n\n\nDuring this period\n\n\nDespite this evolution in treatment\n\n\nAs a result of these conservative approaches\n\n\n\n\n\n\nTransitions are important for several reasons:\n\nThey tell readers how to situate the information they are about to read relative to the information they’ve just read\nThey break up the rhythm of sentences\nThey don’t force your reader to infer continuity between sentences: the reader is doing enough work already and they might not follow your intended inferences.\n\nConsider the differences between transitions like “Furthermore” and “Conversely.” Furthermore tells your reader that you are continuing on with a similar theme – you are establishing that the upcoming sentence is supporting the concept introduced in your previous sentence(s). On the other hand, Conversely establishes a disjunction between the upcoming sentence and the previous sentence(s).\nIf you do not tell the reader how the two sentences are related, they may infer the wrong relationship – particularly if they are unfamiliar with the topic!\n\n\nTransitions should occur either before the subject and verb or between the subject and verb – not after! Once the reader hits the verb, the transition is too late to assist him/her in making projections about the content of the sentence.\nConsider the following example:\n\n“As standard practice, many clinicians are upgrading conventional pacemakers to biventricular pacemakers in patients who subsequently develop heart failure. There are little clinical data to support this practice, however.”\n\nNotice that the transition, “however,” is contained at the end of the sentence (after the verb). Including the transition in this location is pointless because the reader has already inferred the relationship between the first and second sentence by the time they reach the transition. If they correctly inferred the relationship, then reading the word is a waste of time. On the other hand, if they incorrectly inferred the relationship, the transition will throw them off be-cause it will contrast the inferred relationship, making them go back and reread the passage (wasting even more time!).\nInstead, the sentence could be written:\n\n“As standard practice, many clinicians are upgrading conventional pacemakers to biventricular pacemakers in patients who subsequently develop heart failure. However, this practice has scant clinical data to support it.”\n\nDid you catch the expletive and passive sentence in the first version?\n\n\n\n\n\n\nFinal Note on Continuity\n\n\n\nContinuity always trumps clarity. If you need to use passive construction to maintain a strong sequence between sentences, use it!"
  },
  {
    "objectID": "labdocs/writing.html#paragraph-structure",
    "href": "labdocs/writing.html#paragraph-structure",
    "title": "Manuscript Writing",
    "section": "Paragraph Structure",
    "text": "Paragraph Structure\nBegin paragraphs with a set of comprehensive review sentences or a paragraph head. These paragraph heads should help the reader anticipate the full scope of the paragraph that follows. The paragraph head should be approximately 1-3 sentences that introduce the reader to the context, broad topic, and finally, the main point of your paragraph. This sentence (or group of sentences) shouldn’t occupy more than 1/3 of the length of the paragraph. In other words, if you’re writing a short para-graph, you shouldn’t need more than one sentence to introduce it. The paragraph head should be as specific as possible when looking ahead. If your paragraph delivers four causes of a condition, mention that you’re going to cover four causes in the paragraph head, rather than using “several” or “numerous.” In addition, the paragraph head should be slightly discontinuous with the body of the paragraph so that readers can perceive a break between the two sections. Lastly, the paragraph head should not introduce information that is included in subsequent paragraphs.\nConsider this example:\n\nAs with the timing of functional deficits, the severity of speech, voice, and swallowing impairments can display extremely variability across patients. Small resections to oral cavity structures may result in minor speech deviations. Larger re-sections are likely to result in greater deviations. Likewise, laryngeal surgeries often result in some degree of dysphonia with the degree of vocal deficits often related to the amount of tissue removed and the success of any reconstruction. Even among patients experiencing total laryngectomy, postsurgical esophageal or artificial voice abilities may vary widely. Similar analogies are found in patients treated with radiation therapy. Greater mucosal and muscle changes result in a higher risk for more severe functional limitations. Increased severity of functional limitations is typically related to the need for an expanded rehabilitation effort. Any rehabilitation effort must consider the nature and extent of patient symptoms and the underlying contributors to those symptoms.\n\nThe paragraph body should support the statement(s) made in the paragraph head and should never introduce a shift in topic or focus. Rather, if a shift in topic/focus is needed, begin a new paragraph that has its own paragraph head.\nIn summary, all paragraphs should…\n\nContain a paragraph head (between 1 and 3 sentences) that primes the reader for what they will read in the para-graph body\nBe at least 3 sentences long\nContain a paragraph body (≥2 sentences) that supports the paragraph head"
  },
  {
    "objectID": "labdocs/writing.html#manuscript-structure",
    "href": "labdocs/writing.html#manuscript-structure",
    "title": "Manuscript Writing",
    "section": "Manuscript Structure",
    "text": "Manuscript Structure\nThe same organization (head & body) used in paragraphs can be applied to entire sections of a manuscript or to the entire manuscript. For example, the introduction of a manuscript functions as a head for the entire article. Thus, the first paragraph should contain a preliminary thesis to inform readers immediately of the article’s focus.\nLikewise, the final paragraph of the introduction should end with the article’s primary focus/significance. Remember stress order: the very last sentence of the last paragraph should contain the article’s thesis sentence since this sentence will be what the reader remembers most. For a review manuscript, this sentence should precisely describe the purpose of the review and the topics that will be covered. The same general principle holds true for the final summary paragraph. Ostensibly, the summary will be the last thing read and will remain in memory the longest. Consequently, this section should clearly repeat (in general terms) the main points of the review that you would like the reader to remember."
  },
  {
    "objectID": "labdocs/table1.html",
    "href": "labdocs/table1.html",
    "title": "Baseline Characteristics Table",
    "section": "",
    "text": "A common task for us is putting together a baseline characteristics table, or “Table 1.” This is usually painful to do by hand, particularly when you often have to do it multiple times whenever a small change is made to a cohort. The following describes one way of automating much of this, using R and the table1() function in the table1 package."
  },
  {
    "objectID": "labdocs/table1.html#packages",
    "href": "labdocs/table1.html#packages",
    "title": "Baseline Characteristics Table",
    "section": "Packages",
    "text": "Packages\n\nTidyverse\nIf you’re not already familiar with it, I think you’ll find tidyverse extremely helpful - it is actually a bundle of packages centered around ‘tidy’ data, which is just a fancy description for data that take the form of 3 rules:\n\nEach variable has its own column\nEach observation has its own row.\nEach value has its own cell.\n\nOutside of time-varying analyses, that’s pretty much exactly the type of analytic dataset we are creating for most of our work.\n\n\nTable1\nIn addition to tidyverse, for the baseline characteristics table, the package table1 gets us pretty close to a final output. If you want to read more, see it’s vignette at the aforementioned link.\n\n\nHaven\nHaven is a tidyverse package that reads SAS datasets directly. Particularly helpful for us since most of our initial data wrangling has to take place in SAS. If you haven’t noticed already, base R is not particularly good at handling huge datasets (absent a lot of memory resources on a VM) because it completely stores them in memory.\n\n\nFlextable\nFlextable will be used to get the final output into a word document. Will explain more on this later.\n\n# if you need to install, you would use:\n# install.packages(c(\"tidyverse\", \"table1\"))\n\n# load the relevant packages\nlibrary(tidyverse)\nlibrary(table1)\nlibrary(haven)\nlibrary(flextable)"
  },
  {
    "objectID": "labdocs/table1.html#load-the-data",
    "href": "labdocs/table1.html#load-the-data",
    "title": "Baseline Characteristics Table",
    "section": "Load the data",
    "text": "Load the data\nHere’s where you load the SAS dataset, using the haven package. First, you assign the object tab1 (can be any name you want it to be) something using the <- assignment operator, and you assign it the dataset returned by the read_sas() function from haven. There’s lots more options with this function than I use below, but the basics usually work well.\n\n\n\n\n\n\nNote\n\n\n\nNote that R has case-sensitive variable/column names. Make your life a lot easier and rename variables to lowercase with the rename_with() function and tolower option, i.e., rename_with(tolower) (see below)\n\n\n\n# note that the filepath needs forward slashes, or two backwards slashes,\n# e.g., \"C:/Users/.../...sas7bdat\" or \"C:\\\\Users\\\\...\\\\...sas7bdat\"\n#\n# tab1 <- read_sas(\"E:/.../.../...sas7bdat\") %>% \n#   rename_with(tolower)"
  },
  {
    "objectID": "labdocs/table1.html#data-wrangling-for-table1",
    "href": "labdocs/table1.html#data-wrangling-for-table1",
    "title": "Baseline Characteristics Table",
    "section": "Data wrangling for table1",
    "text": "Data wrangling for table1\nI already have my data loaded in an object called aim1cohort, so I’m just going to assign it the new name tab1, but you would not need to run this if you run above to load the data directly from a SAS dataset.\n\n\nCode\nload(file = \"/Users/stevensmith/Dropbox (UFL)/R Projects/K01-Initial_Antihtn_Prescribing/data/aim1cohort.rda\")\n\ntab1 <- aim1cohort \n\n\n\n\n\n\n\n\nA note on ’pipe’s\n\n\n\nOne thing that’s worth reading up on the %>% “pipe”. See this here or here. It’s a tidyverse thing originally coming from the magrittr package. Because it’s quite popular, Base R has now incorporated its own pipe now which does almost exactly the same thing, but looks like this: |\\. You can use ctrl+shift+M as a short-cut, and in your R-studio preferences, you can tell R-Studio whether to use the native pipe |> or magrittr’s %>%. Basically, it’s a way to pipe an object forward from one function to the next, as opposed to having to nest a bunch of functions within one another.\n\n\nOk, here’s the data wrangling code:\n\ntab1 <- tab1 %>%\n  \n  # select only variables needed; ends_with() function is a nifty short cut to \n  # grab all variables whose name ends with that string\n  select(c(\"patid\",\"source\",\"age\",\"age_cat\", \"hispanic\",\"sex\",\"race\",\"index_year\", \n           ends_with(\"indicator\"), \"combined_score_num\")) %>%\n  \n  # mutate() is for creating new variables. Some of this should look pretty \n  # similar to what you're used to in SAS. \n  # c() function just combines multiple things and works similarly here to a\n  # SAS parenthetical list, i.e., race in (\"No Information\", ...)\n  mutate(race = if_else(race %in% c(\"No Information\",\"Refuse to Answer\",\"Unknown\"), \"Unknown\", race),\n         \n         # factor() is a vector type for categorical data and it's important for\n         # table1 package because it's how table1 figures out what is categorical\n         # vs. continuous, and as we'll see below, how to order them in the output. \n         hispanic = factor(hispanic),\n         sex = factor(sex),\n         index_year = factor(index_year),\n         statinindicator = factor(statinindicator),\n         aspirinindicator = factor(aspirinindicator),\n         smokingindicator = factor(smokingindicator),\n         diabetesindicator = factor(diabetesindicator),\n         ckdindicator = factor(ckdindicator),\n         esrdindicator = factor(esrdindicator),\n         hfejindicator = factor(hfejindicator),\n         chdindicator = factor(chdindicator),\n         pcrindicator = factor(pcrindicator),\n         strokeindicator = factor(strokeindicator),\n         padindicator = factor(padindicator),\n         ascvdindicator = factor(ascvdindicator),\n         afindicator = factor(afindicator),\n         copdindicator = factor(copdindicator),\n         asthmaindicator = factor(asthmaindicator),\n         depressionindicator = factor(depressionindicator),\n         goutindicator = factor(goutindicator),\n         source = factor(source),\n         anticogindicator = factor(anticogindicator),\n         ktindicator = factor(ktindicator),\n         osaindicator = factor(osaindicator)\n  ) %>%\n  # arrange() sorts.\n  arrange(patid) %>%\n  # distinct() picks out distinct values, here of patid.\n  distinct(patid, .keep_all = TRUE)\n\n\n# here we go in and work on specific columns of `tab1` dataset and \n# order the levels (values) of that column in the way we want it presented in\n# the output table, using the factor() function. Basically, we're just taking \n# the column as is, and replacing it with the same data, but telling R what is \n# should be the intrinsic order of these values when R outputs anything with it. \n# (Not actually changing given values for a given observation)\ntab1$race <- factor(tab1$race, levels = c(\"American Indian or Alaska Native\",\n                                          \"Asian\",\n                                          \"Black or African American\",\n                                          \"Native Hawaiian or Other Pacific\",\n                                          \"White\",\n                                          \"Multiple Race\",\n                                          \"Other\",\n                                          \"Unknown\"))\ntab1$age_cat <- factor(tab1$age_cat, levels = c(\"<45 y\", \"45-64 y\", \">65 y\"))\ntab1$hispanic <- factor(tab1$hispanic, levels = c(\"Hispanic\", \"Not Hispanic\", \"Unknown\"))\n\n# note that above I'm using base R coding, not tidyverse syntax. \n# I could have accomplished the above with tidyverse (dplyr) syntax also:\ntab1 <- tab1 %>% \n  mutate(race = factor(race, levels = c(\"American Indian or Alaska Native\",\n                                        \"Asian\",\n                                        \"Black or African American\",\n                                        \"Native Hawaiian or Other Pacific\",\n                                        \"White\",\n                                        \"Multiple Race\",\n                                        \"Other\",\n                                        \"Unknown\")),\n         age_cat = factor(age_cat, levels = c(\"<45 y\", \"45-64 y\", \">65 y\")),\n         hispanic = factor(hispanic, levels = c(\"Hispanic\", \"Not Hispanic\", \"Unknown\")),\n         ckdindicator = as.integer(ckdindicator))"
  },
  {
    "objectID": "labdocs/table1.html#labels-and-units",
    "href": "labdocs/table1.html#labels-and-units",
    "title": "Baseline Characteristics Table",
    "section": "Labels and Units",
    "text": "Labels and Units\nHere we can apply label and unit attributes to each column. Labels will be printed (in the output table) as specified here, and will be appended with units, if they’re assigned. Note here I only assign one unit (to age), as most everything else is categorical. But, for example, BP would need units also if included.\n\n# Labels\nlabel(tab1$age) <- \"Age\"\nlabel(tab1$age_cat) <- \"Age Category\"\nlabel(tab1$sex) <- \"Sex\"\nlabel(tab1$race) <- \"Race\"\nlabel(tab1$hispanic) <-  \"Ethnicity\"\nlabel(tab1$smokingindicator) <- \"Current Smoker\"\nlabel(tab1$diabetesindicator) <- \"Diabetes\"\nlabel(tab1$ckdindicator) <- \"Chronic kidney disease\"\nlabel(tab1$esrdindicator) <- \"End-stage renal disease\"\nlabel(tab1$hfejindicator) <- \"Heart failure w/ reduced EF\"\nlabel(tab1$chdindicator) <- \"Coronary heart disease\"\nlabel(tab1$pcrindicator) <- \"Prior coronary revascularization\"\nlabel(tab1$strokeindicator) <- \"Prior stroke or TIA\"\nlabel(tab1$padindicator) <- \"Peripheral arterial disease\"\nlabel(tab1$ascvdindicator) <- \"History of clinical ASCVD\"\nlabel(tab1$afindicator) <- \"Atrial fibrillation\"\nlabel(tab1$copdindicator) <- \"Chronic obstructive pulmonary disease\"\nlabel(tab1$asthmaindicator) <- \"Asthma\"\nlabel(tab1$depressionindicator) <- \"Depression\"\nlabel(tab1$combined_score_num) <- \"Combined Comorbidity Score\"\nlabel(tab1$statinindicator) <- \"Statin\"\nlabel(tab1$aspirinindicator) <- \"Aspirin\"\nlabel(tab1$index_year) <- \"Index Year\"\n\n# Units\nunits(tab1$age) <- \"years\""
  },
  {
    "objectID": "labdocs/table1.html#setting-up-the-output",
    "href": "labdocs/table1.html#setting-up-the-output",
    "title": "Baseline Characteristics Table",
    "section": "Setting up the output",
    "text": "Setting up the output\nHere we tell table1 to use our label list, as well as what columns to give us. This is important for stratified columns (e.g., Medicaid and Medicare, or those with EHR and those without EHR data). I think you can do as many strata as you want, though obviously a lot will not look good in the table.\n\n#### Render Table 1 ####\n# Setup\n\n# here, the first string is the column header, and following the = sign is \n# how to get just the patients that should be used for that column. \n# so for everyone, we use the entire tab1 dataset. For Medicaid column, we \n# subset() tab1 to get only those people who have source = \"FLM\"\nstrata_t1 <- c(list(\"Overall Cohort\" = tab1),\n               list(\"Medicaid-Insured\" = subset(tab1, source == \"FLM\")),\n               list(\"Medicare-Insured\" = subset(tab1, source == \"MED\")))\n\n# tell table1 where to get our labels\nlabels_t1 <- list(\n  variables = list(age = render.varlabel(tab1$age),\n                   age_cat = render.varlabel(tab1$age_cat),\n                   sex = render.varlabel(tab1$sex),\n                   race = render.varlabel(tab1$race),\n                   hispanic = render.varlabel(tab1$hispanic),\n                   smokingindicator = render.varlabel(tab1$smokingindicator),\n                   diabetesindicator = render.varlabel(tab1$diabetesindicator),\n                   ckdindicator = render.varlabel(tab1$ckdindicator),\n                   esrdindicator = render.varlabel(tab1$esrdindicator),\n                   hfejindicator = render.varlabel(tab1$hfejindicator),\n                   chdindicator = render.varlabel(tab1$chdindicator),\n                   pcrindicator = render.varlabel(tab1$pcrindicator),\n                   strokeindicator = render.varlabel(tab1$strokeindicator),\n                   padindicator = render.varlabel(tab1$padindicator),\n                   ascvdindicator = render.varlabel(tab1$ascvdindicator),\n                   afindicator = render.varlabel(tab1$afindicator),\n                   copdindicator = render.varlabel(tab1$copdindicator),\n                   asthmaindicator = render.varlabel(tab1$asthmaindicator),\n                   depressionindicator = render.varlabel(tab1$depressionindicator),\n                   combined_score_num = render.varlabel(tab1$combined_score_num),\n                   statinindicator = render.varlabel(tab1$statinindicator),\n                   aspirinindicator = render.varlabel(tab1$aspirinindicator),\n                   index_year = render.varlabel(tab1$index_year)\n  ))"
  },
  {
    "objectID": "labdocs/table1.html#some-functions-to-style-the-output",
    "href": "labdocs/table1.html#some-functions-to-style-the-output",
    "title": "Baseline Characteristics Table",
    "section": "Some functions to style the output",
    "text": "Some functions to style the output\nBasically here we’re just making some stylistic choices about what we want output to look like for categorical variables, continuous variables, and for the column headers. See the URL above for more of a description of these. But, you probably don’t need to edit this at all for any tables you create.\n\n# add commas to Ns and cell counts\nrender.continuous <- function(x, ...) {\n  with(stats.default(x, ...), c(\"\", \"Mean \\u00B1 SD\"  = sprintf(\"%s \\u00B1 %s\", signif_pad(MEAN, 3, big.mark=\",\"), signif_pad(SD, 3, big.mark=\",\"))))\n}\n\nrender.categorical <- function(x, ...) {\n  c(\"\", sapply(stats.apply.rounding(stats.default(x)), function(y) with(y, sprintf(\"%s (%s%%)\", prettyNum(FREQ, big.mark=\",\"), PCT))))\n}\n\nrender.strat <- function(label, n, ...) {\n  sprintf(\"<span class='stratlabel'>%s<br><span class='stratn'>(N=%s)</span></span>\", label, prettyNum(n, big.mark=\",\"))\n}"
  },
  {
    "objectID": "labdocs/table1.html#render-the-table",
    "href": "labdocs/table1.html#render-the-table",
    "title": "Baseline Characteristics Table",
    "section": "Render the Table",
    "text": "Render the Table\nNow create the actual table.\n\n# Render Table 1 -- Will need to Save the HTML for export off ResVault\ntable1(strata_t1, \n       labels_t1, \n       droplevels = TRUE, \n       # these next three lines are just applying the functions we created above\n       # basically can be read as \"for rendering continuous variables, use the \n       # function render.continuous()\", etc. \n       render.continuous = render.continuous, \n       render.strat = render.strat, \n       render.categorical = render.categorical)\n\n\n\n\n\n\nOverall Cohort(N=143,054)\nMedicaid-Insured(N= 71,774)\nMedicare-Insured(N= 71,280)\n\n\n\n\nAge (years)\n\n\n\n\n\nMean ± SD\n59.1 ± 21.6\n46.5 ± 14.3\n71.8 ± 20.1\n\n\nAge Category\n\n\n\n\n\n<45 y\n38,213 (26.7%)\n33,176 (46.2%)\n5,037 (7.1%)\n\n\n45-64 y\n51,007 (35.7%)\n34,274 (47.8%)\n16,733 (23.5%)\n\n\n>65 y\n53,834 (37.6%)\n4,324 (6.0%)\n49,510 (69.5%)\n\n\nSex\n\n\n\n\n\nFemale\n81,555 (57.0%)\n43,011 (59.9%)\n38,544 (54.1%)\n\n\nMale\n61,493 (43.0%)\n28,760 (40.1%)\n32,733 (45.9%)\n\n\nUnknow\n6 (0.0%)\n3 (0.0%)\n3 (0.0%)\n\n\nRace\n\n\n\n\n\nAmerican Indian or Alaska Native\n334 (0.2%)\n165 (0.2%)\n169 (0.2%)\n\n\nAsian\n1,447 (1.0%)\n659 (0.9%)\n788 (1.1%)\n\n\nBlack or African American\n33,814 (23.6%)\n22,041 (30.7%)\n11,773 (16.5%)\n\n\nNative Hawaiian or Other Pacific\n33 (0.0%)\n11 (0.0%)\n22 (0.0%)\n\n\nWhite\n70,156 (49.0%)\n25,304 (35.3%)\n44,852 (62.9%)\n\n\nMultiple Race\n503 (0.4%)\n167 (0.2%)\n336 (0.5%)\n\n\nOther\n20,555 (14.4%)\n11,610 (16.2%)\n8,945 (12.5%)\n\n\nUnknown\n16,212 (11.3%)\n11,817 (16.5%)\n4,395 (6.2%)\n\n\nEthnicity\n\n\n\n\n\nHispanic\n22,680 (15.9%)\n13,016 (18.1%)\n9,664 (13.6%)\n\n\nNot Hispanic\n99,483 (69.5%)\n44,696 (62.3%)\n54,787 (76.9%)\n\n\nUnknown\n16,203 (11.3%)\n11,865 (16.5%)\n4,338 (6.1%)\n\n\nMissing\n4688 (3.3%)\n2197 (3.1%)\n2491 (3.5%)\n\n\nCurrent Smoker\n\n\n\n\n\n0\n110,974 (77.6%)\n54,739 (76.3%)\n56,235 (78.9%)\n\n\n1\n32,080 (22.4%)\n17,035 (23.7%)\n15,045 (21.1%)\n\n\nDiabetes\n\n\n\n\n\n0\n113,421 (79.3%)\n57,972 (80.8%)\n55,449 (77.8%)\n\n\n1\n29,633 (20.7%)\n13,802 (19.2%)\n15,831 (22.2%)\n\n\nChronic kidney disease\n\n\n\n\n\nMean ± SD\n1.12 ± 0.329\n1.07 ± 0.257\n1.18 ± 0.380\n\n\nEnd-stage renal disease\n\n\n\n\n\n0\n141,982 (99.3%)\n71,541 (99.7%)\n70,441 (98.8%)\n\n\n1\n1,072 (0.7%)\n233 (0.3%)\n839 (1.2%)\n\n\nHeart failure w/ reduced EF\n\n\n\n\n\n0\n140,196 (98.0%)\n70,510 (98.2%)\n69,686 (97.8%)\n\n\n1\n2,858 (2.0%)\n1,264 (1.8%)\n1,594 (2.2%)\n\n\nCoronary heart disease\n\n\n\n\n\n0\n135,114 (94.4%)\n69,032 (96.2%)\n66,082 (92.7%)\n\n\n1\n7,940 (5.6%)\n2,742 (3.8%)\n5,198 (7.3%)\n\n\nPrior coronary revascularization\n\n\n\n\n\n0\n142,326 (99.5%)\n71,632 (99.8%)\n70,694 (99.2%)\n\n\n1\n728 (0.5%)\n142 (0.2%)\n586 (0.8%)\n\n\nPrior stroke or TIA\n\n\n\n\n\n0\n140,762 (98.4%)\n71,465 (99.6%)\n69,297 (97.2%)\n\n\n1\n2,292 (1.6%)\n309 (0.4%)\n1,983 (2.8%)\n\n\nPeripheral arterial disease\n\n\n\n\n\n0\n133,666 (93.4%)\n70,291 (97.9%)\n63,375 (88.9%)\n\n\n1\n9,388 (6.6%)\n1,483 (2.1%)\n7,905 (11.1%)\n\n\nHistory of clinical ASCVD\n\n\n\n\n\n0\n125,638 (87.8%)\n67,545 (94.1%)\n58,093 (81.5%)\n\n\n1\n17,416 (12.2%)\n4,229 (5.9%)\n13,187 (18.5%)\n\n\nAtrial fibrillation\n\n\n\n\n\n0\n133,030 (93.0%)\n70,399 (98.1%)\n62,631 (87.9%)\n\n\n1\n10,024 (7.0%)\n1,375 (1.9%)\n8,649 (12.1%)\n\n\nChronic obstructive pulmonary disease\n\n\n\n\n\n0\n135,285 (94.6%)\n67,186 (93.6%)\n68,099 (95.5%)\n\n\n1\n7,769 (5.4%)\n4,588 (6.4%)\n3,181 (4.5%)\n\n\nAsthma\n\n\n\n\n\n0\n136,988 (95.8%)\n66,919 (93.2%)\n70,069 (98.3%)\n\n\n1\n6,066 (4.2%)\n4,855 (6.8%)\n1,211 (1.7%)\n\n\nDepression\n\n\n\n\n\n0\n117,364 (82.0%)\n61,104 (85.1%)\n56,260 (78.9%)\n\n\n1\n25,690 (18.0%)\n10,670 (14.9%)\n15,020 (21.1%)\n\n\nCombined Comorbidity Score\n\n\n\n\n\nMean ± SD\n2.54 ± 3.53\n1.57 ± 2.82\n3.51 ± 3.89\n\n\nStatin\n\n\n\n\n\n0\n111,633 (78.0%)\n58,573 (81.6%)\n53,060 (74.4%)\n\n\n1\n31,421 (22.0%)\n13,201 (18.4%)\n18,220 (25.6%)\n\n\nAspirin\n\n\n\n\n\n0\n131,391 (91.8%)\n65,659 (91.5%)\n65,732 (92.2%)\n\n\n1\n11,663 (8.2%)\n6,115 (8.5%)\n5,548 (7.8%)\n\n\nIndex Year\n\n\n\n\n\n2012\n32 (0.0%)\n1 (0.0%)\n31 (0.0%)\n\n\n2013\n17,728 (12.4%)\n485 (0.7%)\n17,243 (24.2%)\n\n\n2014\n31,711 (22.2%)\n11,457 (16.0%)\n20,254 (28.4%)\n\n\n2015\n27,205 (19.0%)\n11,039 (15.4%)\n16,166 (22.7%)\n\n\n2016\n19,283 (13.5%)\n10,664 (14.9%)\n8,619 (12.1%)\n\n\n2017\n18,661 (13.0%)\n9,694 (13.5%)\n8,967 (12.6%)\n\n\n2018\n8,963 (6.3%)\n8,963 (12.5%)\n0 (0%)\n\n\n2019\n8,164 (5.7%)\n8,164 (11.4%)\n0 (0%)\n\n\n2020\n7,520 (5.3%)\n7,520 (10.5%)\n0 (0%)\n\n\n2021\n3,787 (2.6%)\n3,787 (5.3%)\n0 (0%)\n\n\n\n\n\n# Can also save Table 1 for posterity, or to export as a CSV\n# would just need to uncomment the following code\n# t1 <- as.data.frame(table1(strata_t1, labels_t1, droplevels = TRUE,\n#                            render.continuous = render.continuous, render.strat = render.strat, render.categorical = render.categorical))\n# \n# write_csv(t1, file = \"path_to_directory/Table 1.csv\")\n\nNote that this output still needs a bit of editing, e.g., copying the data in the ‘1’ row for the various indicators, up to the the row with the respective label, and then deleting the ‘0’ row and what was the ‘1’ row. I think a modification can probably be built into the package to do this routinely, or as an option, but I haven’t had time to fool with it. In any event, this gets us pretty close."
  },
  {
    "objectID": "labdocs/table1.html#getting-this-into-word",
    "href": "labdocs/table1.html#getting-this-into-word",
    "title": "Baseline Characteristics Table",
    "section": "Getting this into Word",
    "text": "Getting this into Word\nOk, we have the table in HTML now, but we need it in Word. A straight copy + paste from R Studio viewer to word document does not work very well, because it either keeps all the HTML formatting (check by selecting all of the pasted table and adding all borders to the table), or it completely loses formatting if you do a Special Paste, i.e., no Word table structure any more.\nHere’s a couple of ways around this (I’m sure there are others):\n\nExport the output in the R Studio viewer to an HTML file on your computer. Then, open that file, hit CTRL+A (or CMD+A on Mac), CTRL + C, then go to Word Document and Paste Special -> HTML. It typically doesn’t look as pretty, but fixes both of the above issues.\nLet Flextable do the heavy lifting for this.\n\nHere, I use flextable. Instead of printing the table1() function results, as above, we can instead save them to an object, named tbl1 below. Then, convert it to a flextable, and have the flextable package export it as a word document.\n\n# same as above, but instead of printing, saving as object. \ntbl1 <- table1(strata_t1, \n               labels_t1, \n               droplevels = TRUE, \n               # these next three lines are just applying the functions we created above\n               # basically can be read as \"for rendering continuous variables, use the \n               # function render.continuous()\", etc. \n               render.continuous = render.continuous, \n               render.strat = render.strat, \n               render.categorical = render.categorical)\n\n# convert to flextable, then save as doc.\n# update path as needed. \nt1flex(tbl1) %>% \n  save_as_docx(path=\"./output/table1_output.docx\")\n\nVoila - here’s the output."
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "CV med Lab",
    "section": "",
    "text": "The CV med Lab is focused on studying adverse cardiovascular effects of common chronic diseases (hypertension, type 2 diabetes, thromboembolic disorders) and optimizing drug therapy to minimize risks of these diseases.\nA major focus is on adverse effects of chronic uncontrolled blood pressure (hypertension) and ways to optimize antihypertensive drug therapy to mitigate these adverse effects. Our particular expertise is in the area of treatment-resistant hypertension.\n\n\n\nOur research spans population and clinical sciences related to cardiovascular disease and its treatment. This includes both observational research, as well as clinical trial work, primarily through collaborations with colleagues in the Division of Cardiovascular Medicine.\nWe are also interested in research on methods for using electronic health record (EHR) data, including how to minimize misclassification when measuring longitudinal drug exposure from EHR prescribing data, and methods for dealing with missingness in EHR data and biases associated with missing data.\n\n\n\nCurrent funding for the lab is through the National Heart, Lung and Blood Institute (NHLBI) via our work on optimizing antihypertensive drug selection (K01 HL138172) and high-throughput screening for antihypertensive prescribing cascades (R21 HL159576).\n\n\n\nA complete list of my lab’s journal publications can be found on our papers page, as well as on PubMed or Google Scholar. Newer publications are also available via ORCID. We provide pdfs of most of our pubs, but if for some reason you cannot access the link, or a particular journal, please email me for an author’s print."
  },
  {
    "objectID": "LICENSE.html",
    "href": "LICENSE.html",
    "title": "CV med Lab",
    "section": "",
    "text": "Copyright (c) 2022 cvmedlab.github.io authors\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the “Software”), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\nTHE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE."
  },
  {
    "objectID": "papers.html",
    "href": "papers.html",
    "title": "Papers",
    "section": "",
    "text": "Lab papers are listed below, sorted (descending) by year. Where available, the PDF link is also listed, as is the github site or website."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CV med Lab",
    "section": "",
    "text": "Background\nI am a pharmacist and epidemiologist specializing in cardiovascular medicine, especially hypertension. I obtained my PharmD in 2007 and MPH, with an emphasis in biostatistics and epidemiology in 2011, both from the University of Florida. I completed a Pharmacy Practice Residency at the NF/SG Veterans Affairs Hospital (2008) and a postdoctoral fellowship in Family Medicine at the University of Florida (2011). I started my career at the University of Colorado in 2011 and returned to the University of Florida in 2014.\n\n\nContact\nmailssmith@cop.ufl.edu\nmailsteve@stevenmsmith.org"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "CV med Lab",
    "section": "",
    "text": "Interested in Joining Us?\nIf you are a current graduate student or pharmacy student at UF and interested in working with our group on comparative effectiveness research for cardiometabolic drugs, in particular antihypertensives, or interested in methods related to use of EHR data in epidemiologic research, please feel free to email me to discuss.\nIf you are a prospective graduate student interested in working with me specifically then send me an email describing your interests along with your CV. I take graduate students in the Pharmacoepidemiology and Safety Sciences program. Note you must meet the qualifications for admittance to the respective PhD program and be accepted into the program. More information is available at the above link.\nIf you are a prospective postdoctoral fellow interested in pharmacoepidemiologic/outcomes research, please contact me directly and include your CV."
  },
  {
    "objectID": "people.html",
    "href": "people.html",
    "title": "CV med Lab",
    "section": "",
    "text": "Steven Smith, PharmD, MPH is an Assistant Professor in the Department of Pharmaceutical Outcomes & Policy (College of Pharmacy) and Division of Cardiovascular Medicine (College of Medicine) at the University of Florida. I also serve as the Associate Director (population sciences) for the UF Center for Integrative Cardiovascular and Metabolic Disease. My research methodology is fairly wide-ranging, encompassing clinical trial work and observational methods (e.g., with claims and electronic health record data), though my therapeutic focus is primarily in the field of hypertension.\n\n\n\n\n\n\n\n\n\n\n\n\nChristie Monahan, PharmD is the 2021-2023 postdoctoral fellow in the Departments of Pharmacotherapy and Translational Research and Community Health and Family Medicine. Her work is primarily centered on health services research around pharmacist-managed transitions of care. She is also involved in our hypertension research.\n\n\n\n\n\n\n\n\n\n\n\n\nShailina Keshwani, MS is a PhD student in the Department of Pharmaceutical Outcomes and Policy. Her research is focused on the intersection of pain and cardiovascular disease and specifically use of analgesics in patients with cardiovascular disease who have contraindications to such medications.\n\n\n\n\n\n\n\n\n\nAsinamai Ndai, MS is a PhD student in the Department of Pharmaceutical Outcomes and Policy. His research is focused on the intersection of hypertension and HIV.\n\n\n\n\n\n\n\n\n\nKayla Smith, PharmD is a PhD student in the Department of Pharmaceutical Outcomes and Policy. She previously worked with me on several project characterizing antihypertensive use patterns during her Doctor of Pharmacy degree program.\n\n\n\n\n\n\n\n\n\n\n\n\nMarta Walsh, MS is our analyst and basically single-handedly manages our messy data and keeps us moving forward. She’s a former UF and Embry-Riddle grad and our shop wouldn’t be what it is without her.\n\n\n\n\n\n\n\n\n\n\n\n\nRaj Desai, PhD was my first PhD student in the Department of Pharmaceutical Outcomes and Policy. His work was primarily centered on use of secondary datasets (administrative claims, EHR) for studying cardiovascular outcomes and performing comparative effectiveness research in patients with complicated antihypertensive regimens. He is now an Associate - Health Economics and Outcomes Research at the Analysis Group in Boston, MA.\n\n\n\n\n\n\n\n\n\nChris Piszczatoski, PharmD was our 2019-2021 postdoctoral fellow in the Departments of Pharmacotherapy and Translational Research and Community Health and Family Medicine. He is now on staff in the UF Community Health and Family Medicine.\n\n\n\n\n\n\n\n\n\nGhadeer Dawwas, PhD was a postdoctoral fellow (2019-2020) in pharmacoepidemiology, with a focus on cardiometabolic outcomes of novel antidiabetic agents. She is now on faculty at Vanderbilt University.\n\n\n\n\n\n\n\n\n\nScott Garland, PharmD was our 2017-2019 postdoctoral fellow in the Departments of Pharmacotherapy and Translational Research and Community Health and Family Medicine. He is now on faculty of the FSU College of Medicine.\n\n\n\n\n\n\n\n\n\nAndrew Hwang, PharmD was our 2015-2017 postdoctoral fellow in the Departments of Pharmacotherapy and Translational Research and Community Health and Family Medicine. He is now on faculty at the Massachusetts College of Pharmacy and Health Sciences.\n\n\n\nSee the about page for information on joining the group."
  }
]